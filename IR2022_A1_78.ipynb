{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPa3GGWYCR_x"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHmJ2-89DAl8",
        "outputId": "c75854d7-dde9-4b91-f713-7a55c7b34318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2C8fcWWR0sa",
        "outputId": "0cba7e1e-67ff-4f72-a1c4-05a37c9fbbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.66)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ],
      "source": [
        "#Installing required packages\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFJXZ8E1R1I3",
        "outputId": "b60f594a-66a9-4067-c3e6-c3afc89b5014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Installing necessary libraries\n",
        "import os\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import json\n",
        "from textblob import TextBlob, Word\n",
        "import joblib\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "lema = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuOiH8FPDBMl",
        "outputId": "85221ef1-c724-419d-a644-1a2c31b0dd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gohome.hum', 'rinaldos.txt', 'mowers.txt', 'harmful.hum', 'hell.jok', 'mothers.txt', 'herb!.hum', 'hate.hum', 'hi.tec', 'roach.asc', 'icm.hum', 'conan.txt', 'howlong.hum', 'murph.jok', 'imprrisk.hum', 'coyote.txt', 'horoscop.jok', 'murphy.txt', 'impurmat.hum', 'incarhel.hum', 'bible.txt', 'dirtword.txt', 'insanity.hum', 'insure.hum', 'cartoon_.txt', 'investi.hum', 'interv.hum', 'bless.bc', 'jac&tuu.hum', 'japrap.hum', 'terms.hum', 'ivan.hum', 'test.jok', 'dym', 'killer.hum', 'blooprs1.asc', 'kilsmur.hum', 'testchri.txt', 'killself.hum', 'lawsuniv.hum', 'test2.jok', 'kid_diet.txt', 'motrbike.jok', 'lbinter.hum', 'murphys.txt', 'ludeinfo.txt', 'legal.hum', 'simp.txt', 'office.txt', 'llong.hum', 'lll.hum', 'livnware.hum', 'lobquad.hum', 'lifeinfo.hum', 'lifeimag.hum', 'lif&love.hum', 'looser.hum', 'phunatdi.ana', 'losers84.hum', 'ludeinfo.hum', 'lozerzon.hum', 'lozeuser.hum', 'catstory.txt', 'losers86.hum', 'luggage.hum', 'm0dzmen.hum', 'makebeer.hum', 'manilla.hum', 'manspace.hum', 'mailfrag.hum', 'maecenas.hum', 'luzerzo2.hum', 'madscrib.hum', 'mash.hum', 'pun.txt', 'mtm.hum', 'meinkamp.hum', 'melodram.hum', 'marines.hum', 'miamadvi.hum', 'miranda.hum', 'miami.hum', 'reasons.txt', 'f_tang.txt', 'memo.hum', 'mutate.hum', 'mrscienc.hum', 'misery.hum', 'calvin.txt', 'missheav.hum', 'montpyth.hum', 'mydaywss.hum', 'newconst.hum', 'boston.geog', 'cars.txt', 'myheart.hum', 'news.hum', 'newmex.hum', 'naivewiz.hum', 'cartoon.law', 'novel.hum', 'nukeplay.hum', 'cartwb.son', 'nurds.hum', 'nuke.hum', 'nysucks.hum', 'odearakk.hum', 'ohandre.hum', 'chunnel.txt', 'oldeng.hum', 'oilgluts.hum', 'onetoone.hum', 'o-ttalk.hum', 'ozarks.hum', 'onetotwo.hum', 'p-law.hum', 'deep.txt', 'ookpik.hum', 'opinion.hum', 'peatchp.hum', 'phony.hum', 'planeget.hum', 'parabl.hum', 'pizzawho.hum', 'phorse.hum', 'passage.hum', 'devils.jok', 'poll2res.hum', 'policpig.hum', 'prawblim.hum', 'popconc.hum', 'popmusi.hum', 'prayer.hum', 'dieter.txt', 'psilaine.hum', 'pro-fact.hum', 'quest.hum', 'dead-r', 'ratspit.hum', 'donut.txt', 'rapmastr.hum', 'ratings.hum', 'fuckyou2.txt', 'reagan.hum', 'raven.hum', 'radiolaf.hum', 'docspeak.txt', 'research.hum', 'repair.hum', 'reddye.hum', 'report.hum', 'drinkrul.jok', 'rentals.hum', 'rocking.hum', 'rockmus.hum', 'ripoffpc.hum', 't_zone.jok', 'dubltalk.jok', 'drunk.txt', 'insult', 'corporat.txt', 'jokes', 'skippy.hum', 'takenote.jok', 'cucumber.txt', 'smackjok.hum', 'shuttleb.hum', 'iqtest', 'smurfkil.hum', 'soleleer.hum', 'social.hum', 'terbear.txt', 'solviets.hum', 'spider.hum', 'teens.txt', 'spydust.hum', 'socecon.hum', 'stone.hum', 'termpoem.txt', 'standard.hum', 'sungenu.hum', 'top10.txt', 't-shirt.hum', 'texican.dic', 'languag.jok', 'taping.hum', 'televisi.hum', 't-10.hum', 'talebeat.hum', \"terrmcd'.hum\", 'y.txt', 'timetr.hum', 'terrnieg.hum', 'tfpoems.hum', 'textgrap.hum', 'test.hum', 'tickmoon.hum', 'tfepisod.hum', 'thecube.hum', 'let.go', 'the_math.hel', 'hecomes.jok', 'koans.txt', 'toxcwast.hum', 'lawyer.jok', 'truthlsd.hum', 'turbo.hum', 'thermite.ana', 'truths.hum', 'tribble.hum', 'cartoon.laws', 'voltron.hum', 'thesis.beh', 'trekwes.hum', 'whoon1st.hum', 'wetdream.hum', 'twinkie.txt', 'wagon.hum', 'whoops.hum', 'why-me.hum', 'wedding.hum', 'yuppies.hum', 'word.hum', 'worldend.hum', 'xtermin8.hum', 'billcat.hum', 'who.txt', 'yjohncse.hum', 'zodiac.hum', 'hotnnot.hum', 'trukdeth.txt', 'jeffie.heh', 'liceprof.sty', 'limerick.jok', 'urban.txt', 'lines.jok', 'tshirts.jok', 'units.mea', 'univ.odd', 'wagit.txt', 'st_silic.txt', 'waitress.txt', 'washroom.txt', 'vaguemag.90s', 'welfare.txt', 'marriage.hum', 'lotsa.jok', 'weight.txt', 'strine.txt', 'lion.jok', 'math.2', 'woodbugs.txt', 'math.1', 'misc.1', 'math.far', 'wimptest.txt', 'yogisays.txt', 'woolly_m.amm', 'chainltr.txt', 'zgtoilet.txt', 'xibovac.txt', 'lozers', 'climbing.let', 'church.sto', 'commutin.jok', 'co-car.jok', 'netnews.10', 'childhoo.jok', 'nigel.10', 'nigel.2', 'court.quips', 'hack', 'eandb.drx', 'nigel.3', 'element.jok', 'elephant.fun', 'electric.txt', 'fartting.txt', 'drinker.txt', 'idr2.txt', 'excuses.txt', 'fartinfo.txt', 'exam.50', 'fascist.txt', 'ourfathr.txt', 'nigel.6', 'jokes.txt', 'nigel.4', 'final-ex.txt', 'nigel.5', 'lawhunt.txt', 'fusion.sup', 'nigel.7', 'fuck!.txt', 'lawskool.txt', 'good.txt', 'grammar.jok', 'passenge.sim', 'gd_guide.txt', 'grospoem.txt', 'german.aut', 'gas.txt', 'rec.por', 'nintendo.jok', 'pepsideg.txt', 'pickup.lin', 'potty.txt', 'polemom.txt', 'poli_t.ics', 'polly.txt', 'proof.met', 'pournell.spo', 'problem.txt', 'dover.poem', 'polly_.new', 'moonshin', 'psalm23.txt', 'psych_pr.quo', 'psalm_re.aga', 'prooftec.txt', 'proposal.jok', 'pure.mat', 'prover_w.iso', 'pukeprom.jok', 'curse.txt', 'python_s.ong', 'tuflife.txt', 'leech.txt', 'puzzles.jok', 'quux_p.oem', 'college.hum', 'peanuts.txt', 'pickup.txt', 'flowchrt.txt', 'height.txt', 'pecker.txt', 'smartass.txt', 'eskimo.nel', 'pipespec.txt', 'hitler.59', 'hilbilly.wri', 'prac1.jok', 'hitlerap.txt', 'steroid.txt', 'prac3.jok', 'htswfren.txt', 'horoscop.txt', 'snipe.txt', 'stereo.txt', 'smokers.txt', 'how_to_i.pro', 'hoosier.txt', 'insuranc.sty', 'law.sch', 'humatran.jok', 'humatra.txt', 'televisi.txt', 'inlaws1.txt', 'idaho.txt', 'sysadmin.txt', 'psycho.txt', 'jokeju07.txt', 'jc-elvis.inf', 'jargon.phd', 'realest.txt', 'woodbine.txt', 'kloo.txt', 'italoink.txt', 'letgosh.txt', 'laws.txt', 'rednecks.txt', 'lampoon.jok', 'sysman.txt', 'letter_f.sch', 'letter.txt', 'farsi.phrase', 'odd_to.obs', 'luvstory.txt', 'nukwaste', 'lions.cat', 'prac4.jok', 'lucky.cha', 'oracle.jok', 'one.par', 'smiley.txt', 'smurfs.cc', 'primes.jok', 'oxymoron.jok', 'signatur.jok', 'socks.drx', 'some_hu.mor', 'spelin_r.ifo', 'princess.brd', 'studentb.txt', 'strsdiet.txt', 'startrek.txt', 'puzzle.spo', 'jrrt.riddle', 'telecom.q', 'humpty.dumpty', 'mr.rogers', 'prover.wisom', 'shrink.news', 'number.killer', 'psalm.reagan', 'poli.tics', 'un.happy', 'popmach', 'rent-a_cat', 'psalm_nixon', 'quick.jok', 'quantum.jok', 'resrch_phrase', 'nosuch_nasfic', 'quotes.jok', 'racist.net', 'progrs.gph', 'rinaldos.law', 'quotes.bug', 'shorties.jok', 'welfare', 'squids.gph', 'tnd.1', 'quotes.txt', 'texican.lex', 'temphell.jok', 'top10.elf', 'aussie.lng', 'bw-phwan.hat', 'bw-summe.hat', 'wisdom', 'firstaid.inf', 'outawork.erl', 'justify', 'la_times.hun', 'speling.msk', 'supermar.rul', 'oldtime.sng', 'ambrose.bie', 'oasis', 'oxymoron.txt', 'gaiahuma', 'kid2', 'anim_lif.txt', 'bad', 'talkbizr.txt', 'wood', 'woodsmok.txt', 'bagelope.txt', 'variety1.asc', 'engrhyme.txt', 'variety2.asc', 'variety3.asc', 'empeval.txt', 'english', 'epitaph', 'various.txt', 'excuse.txt', 'excuse30.txt', 'vegkill.txt', 'quack26.txt', 'q.pun', 'vonthomp', 'godmonth.txt', 'gown.txt', 'growth.txt', 'cogdis.txt', 'b12.txt', 'mrsfield', 'brewing', 'coke1', 'tuna.lab', 'coladrik.fun', 'vegan.rcp', 'chili.txt', 'bakebred.txt', 'coke.txt', 'snapple.rum', 'beginn.ers', 'meat2.txt', 'jerky.rcp', 'hamburge.nam', 'candybar.fun', 'coladrik.txt', 'mead.rcp', 'newcoke.txt', 'cooking.fun', 'coke.fun', 'coke_fan.naz', 'x-drinks.txt', 'candy.txt', 'turkey.fun', 'recepies.fun', 'choco-ch.ips', 'bread.txt', 'fiber.txt', 'kashrut.txt', 'oculis.rcp', 'curry.hrb', 'pepper.txt', 'fajitas.rcp', 'fudge.txt', 'oatbran.rec', 'appetiz.rcp', 'drinks.gui', 'aclamt.txt', 'hotel.txt', 'boneles2.txt', 'dthought.txt', 'tpquote2.txt', 'tpquotes.txt', 'gingbeer.txt', 'gameshow.txt', 'cooking.jok', 'rinaldo.jok', 'women.jok', 'wrdnws1.txt', 'wrdnws2.txt', 'wrdnws3.txt', 'wrdnws4.txt', 'wrdnws5.txt', 'wrdnws6.txt', 'wrdnws7.txt', 'wrdnws8.txt', 'wrdnws9.txt', 'letterbx.txt', 'hierarch.txt', 'ads.txt', 'epikarat.txt', 'namm', 'gd_flybd.txt', 'earp', 'adt_miam.txt', 'kanalx.txt', 'widows', 'twinpeak.txt', 'hoonsrc.txt', 'lazarus.txt', 'barney.cn1', 'barney.txt', 'bb', 'bimg.prn', 'bmdn01.txt', 'libraway.txt', 'wisconsi.txt', 'twinkies.jok', 'sanshop.txt', 'scam.txt', 'seeds42.txt', 'top10st1.txt', 'top10st2.txt', 'transp.txt', 'mundane.v2', 'japantv.txt', 'johann', 'just2', 'bnb_quot.txt', 'bored.txt', 'sw_err.txt', 'skippy.txt', 'recip1.txt', 'shooters.txt', 'slogans.txt', 'smurf-03.txt', 'smurf_co.txt', 'soccer.txt', 'beerjesus.hum', 'bozo_tv.leg', 'coffee.txt', 'diet.txt', 'jason.fun', 'econridl.fun', 'oliver.txt', 'oliver02.txt', 'sf-zine.pub', 'subrdead.hum', 'topten.hum', 'how2bgod.txt', 'hangover.txt', 'netmask.txt', 'chickens.txt', 'classicm.hum', 'cmu.share', 'commword.hum', 'cookbkly.how', 'fearcola.hum', 'hedgehog.txt', 'horflick.txt', 'alcohol.hum', 'bbh_intv.txt', 'beer-g', 'beergame.hum', 'dining.out', 'iced.tea', 'thievco.txt', 'aeonint.txt', 'mindvox', 'deathhem.txt', 'bread.rec', 'brownie.rec', 'lion.txt', 'lp-assoc.txt', 'desk.txt', 'disclym.txt', 'flowchrt', 'flux_fix.txt', 'packard.txt', 'parsnip.txt', 'penisprt.txt', 'msfields.txt', 'mtv.asc', 'modstup', 'moslem.txt', 'mov_rail.txt', 'miamimth.txt', 'missdish', 'consp.txt', 'food', 'foodtips', 'cake.rec', 'docdict.txt', 'drinks.txt', 'drive.txt', 'bhang.fun', 'booze1.fun', 'booze2.fun', 'cgs_lst.txt', 'golnar.txt', 'red-neck.jks', 'reddwarf.sng', 'renorthr.txt', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'robot.tes', 'subb_lis.txt', 'staff.txt', 'stuf10.txt', 'stuf11.txt', 'fusion.gal', 'sfmovie.txt', 'the_ant.txt', 'mog-history', 'dalive', 'iremember', 'kilroy', 'cform2.txt', 'msorrow', 'nameisreo.txt', 'inquirer.txt', 'ins1', 'bnbguide.txt', 'buldrwho.txt', 'epi_merm.txt', 'epi_.txt', 'epi_bnb.txt', 'episimp2.txt', 'epi_tton.txt', 'gd_alf.txt', 'epi_rns.txt', 'outlimit.txt', 'scratchy.txt', 'lost.txt', 'gd_drwho.txt', 'twilight.txt', 'gd_gal.txt', 'epiquest.txt', 'gd_maxhd.txt', 'gd_liqtv.txt', 'gd_hhead.txt', 'gd_frasr.txt', 'gd_ol.txt', 'ghostsch.hum', 'gd_sgrnd.txt', 'gd_ql.txt', 'gd_tznew.txt', 'a-team', 'a_fish_c.apo', 'a_tv_t-p.com', 'aboutada.txt', 'adrian_e.faq', 'allfam.epi', 'allusion', 'amazing.epi', 'anime.cli', 'anime.lif', 'hbo_spec.rev', 'highland.epi', 'hitchcok.txt', 'wacky.ani', 'wkrp.epi', 'cast.lis', 'christop.int', 'chung.iv', 'clancy.txt', 'comic_st.gui', 'cultmov.faq', 'maxheadr', 'facedeth.txt', 'farsi.txt', 'nukewar.txt', 'number', 'spoonlis.txt', 'sinksub.txt', 'sorority.gir', 'petshop', 'prac2.jok', 'pracjoke.txt', 'fed.txt', 'cybrtrsh.txt', 'hstlrtxt.txt', 'homermmm.txt', 'how2dotv.txt', 'filmgoof.txt', 'films_gl.txt', 'fish.rec', 'hitchcoc.app', 'is_story.txt', 'fegg!int.txt', 'feggaqui.txt', 'feggmagi.txt', 'normquot.txt', 'history2.oop', 'pol-corr.txt', 'poopie.txt', 'ppbeer.txt', 'pot.txt', 'headlnrs', 'hell.txt', 'hum2', 'humor9.txt', 'cokeform.txt', 'contract.moo', 'cookberk', 'cookie.1', 'cooplaws', 'cuisine.txt', 'coffee.faq', 'modemwld.txt', 'minn.txt', 'oam-001.txt', 'oam.nfo', 'acronym.txt', 'college.txt', 'english.txt', 'figure_1.txt', 'crzycred.lst', 'arnold.txt', 'ateam.epi', 'avengers.lis', 'bbc_vide.cat', 'beauty.tm', 'blackadd', 'blake7.lis', 'doc-says.txt', 'dromes.txt', 'eatme.txt', 'free-cof.fee', 'initials.rid', 'old.txt', 'post.nuc', 'quantum.phy', 'resolutn.txt', 'soporifi.abs', 'swearfrn.hum', 'tarot.txt', 'c0dez.txt', 'beer.txt', 'butcher.txt', 'curiousgeorgie.txt', 'freudonseuss.txt', 'normalboy.txt', 'annoy.fascist', 'booze.fun', 'diesmurf.txt', 'elevator.fun', 'get.drunk.cheap', 'how.bugs.breakd', 'normal.boy', 'practica.txt', 'dead2.txt', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'radexposed.txt', 'carowner.txt', 'pat.txt', 'bugs.txt', 'hack7.txt', 'happyhack.txt', 'airlines', 'skincat', 'venganza.txt', 'venison.txt', 'homebrew.txt', 'hop.faq', 'hotpeper.txt', 'necropls.txt', 'three.txt', 'eggroll1.mea', 'dandwine.bev', 'egg-bred.txt', 'egglentl.vgn', 'engmuffn.txt', 'feista01.dip', 'focaccia.brd', 'frogeye1.sal', 'firecamp.txt', 'goldwatr.txt', 'texbeef.txt', 'garlpast.vgn', 'greenchi.txt', 'unochili.txt', 'wonton.txt', 'whitbred.txt', 'gack!.txt', 'woods.txt', 'yogurt.asc', 'insult.lst', 'insults1.txt', 'damiana.hrb', 'calamus.hrb', 'imbecile.txt', 'pbcookie.des', 'pasta001.sal', 'penndtch', 'oranchic.pol', 'orgfrost.bev', 'jon.txt', 'hackmorality.txt', 'whatbbs', 'hermsys.txt', 'appbred.brd', 'applepie.des', 'awespinh.sal', 'antimead.bev', 'apsaucke.des', 'arcadian.txt', 'banana02.brd', 'banana01.brd', 'beershrm.fis', 'beershrp.fis', 'banana04.brd', 'banana05.brd', 'batrbred.txt', 'banana03.brd', 'baklava.des', 'jawsalad.fis', 'japice.bev', 'jambalay.pol', 'berryeto.bev', 'jawgumbo.fis', 'jalapast.dip', 'jungjuic.bev', 'mitch.txt', 'zuccmush.sal', 'shuimai.txt', 'strattma.txt', 'margos.txt', 'montoys.txt', 'stagline.txt', 'zucantom.sal', 'seafood.txt', 'blkbean.txt', 'breadpud.des', 'brdpudd.des', 'bread.rcp', 'btscke03.des', 'btscke01.des', 'btscke02.des', 'bredcake.des', 'blkbnsrc.vgn', 'boarchil.txt', 'buffwing.pol', 'burrito.mea', 'btscke04.des', 'butstcod.fis', 'btscke05.des', 'capital.txt', 'bunacald.fis', 'caesardr.sal', 'caramels.des', 'advrtize.txt', 'bitnet.txt', 'nzdrinks.txt', 'recipe.010', 'recipe.002', 'recipe.011', 'recipe.005', 'richbred.txt', 'recipe.004', 'recipe.012', 'oldtime.txt', 'oakwood.txt', 'recipe.001', 'recipe.007', 'recipe.008', 'recipe.009', 'recipe.006', 'recipe.003', 'renored.txt', 'all_grai', 'amchap2.txt', 'beer.gam', 'beergame.txt', 'beer-gui', 'gotukola.hrb', 'hitler.txt', 'coollngo2.txt', 'jimhood.txt', 'acne1.txt', 'curry.txt', 'ayurved.txt', 'arthriti.txt', 'beesherb.txt', 'brush1.txt', 'blood.txt', 'cereal.txt', 'acetab1.txt', 'atherosc.txt', '1st_aid.txt', 'critic.txt', 'back1.txt', 'booknuti.txt', 'aniherb.txt', 'chinese.txt', 'aphrodis.txt', 'anorexia.txt', 'antibiot.txt', 'bw.txt', 'proudlyserve.txt', 'jayjay.txt', 'bnbeg2.4.txt', 'att.txt', 'calculus.txt', 'gumbo.txt', 'btaco.txt', 'byfb.txt', 'beerwarn.txt', 'qttofu.vgn', 'paddingurpapers.txt', 'childrenbooks.txt', 'cartoon_laws.txt', 'collected_quotes.txt', 'coffeebeerwomen.txt', 'aggie.txt', 'llamas.txt', 'trekfume.txt', 'fireplacein.txt', 'ganamembers.txt', 'horoscope.txt', 'confucius_say.txt', 'labels.txt', 'girlspeak.txt', 'enlightenment.txt', 'religion.txt', 'turing.shr', 'valujet.txt', 'computer.txt', 'jokes1.txt', 'lawyers.txt', 'cops.txt', 'fbipizza.txt', 'mcd.txt', 'nasaglenn.txt', 'bhb.ill', 'bond-2.txt', 'indgrdn.txt', 'insect1.txt', 'quantity.001', 'sawyer.txt', 'exidy.txt', 'deadlysins.txt', 'yuban.txt', 'planetzero.txt', 'aids.txt', 'reeves.txt', 'moore.txt', 'chickenheadbbs.txt', 'phxbbs-m.txt', 'ukunderg.txt', 'silverclaws.txt', 'namaste.txt', 'crazy.txt', 'lifeonledge.txt', 'lipkovits.txt', 'basehead.txt', 'draxamus.txt', 'stressman.txt', 'exylic.txt', 'apsnet.txt', 'lansing.txt', 'heroic.txt', 'onan.txt', 'teevee.hum', 'mel.txt', 'grail.txt', 'disclmr.txt', 'hacktest.txt', 'hackingcracking.txt', 'bingbong.hum', 'charity.hum', 'adameve.hum', 'analogy.hum', 'modest.hum', 'drugshum.hum', 'mlverb.hum', 'parades.hum', 'saveface.hum', 'ghostfun.hum', 'reconcil.hum', 'poets.hum', 'cheapfar.hum', 'brainect.hum', 'shameonu.hum', 'watchlip.hum', 'whatthe.hum', 'weights.hum', 'throwawa.hum', 'solders.hum', 'symbol.hum', 'memory.hum', 'kaboom.hum', 'hammock.hum', 'spacever.hum', 'sigs.txt', 'adcopy.hum', 'zen.txt', 'nigel.1', 'b-2.jok', 'manager.txt', 'abbott.txt', 'admin.txt', 'alcatax.txt', 'suicide2.txt', 'acronym.lis', 'atombomb.hum', 'alflog.txt', 'nigel10.txt', 'cancer.rat', 'bank.rob', 'answers', 'boatmemo.jok', 'badday.hum', 'alabama.txt', 'beapimp.hum', 'beave.hum', 'catin.hat', 'beer.hum', 'bigpic1.hum', 'bitchcar.hum', 'blackapp.hum', 'argotdic.txt', 'browneco.hum', 'blaster.hum', 'cheapin.la', 'cold.fus', 'blackhol.hum', 'boe.hum', 'cabbage.txt', 'buzzword.hum', 'bugbreak.hum', 'btcisfre.hum', 'butwrong.hum', 'chickens.jok', 'merry.txt', 'catballs.hum', 'calif.hum', 'nihgel_8.9', 'cockney.alp', 'change.hum', 'chinesec.hum', 'cbmatic.hum', 'macsfarm.old', 'catranch.hum', 'making_y.wel', 'nukewar.jok', 'college.sla', 'madhattr.jok', 'coldfake.hum', 'malechem.txt', 'cucumber.jok', 'arab.dic', 'number_k.ill', 'comrevi1.hum', 'manners.txt', 'readme.bat', 'addrmeri.txt', 'rabbit.txt', 'anthropo.stu', 'cowexplo.hum', 'cuchy.hum', 'failure.txt', 'd-ned.hum', 'dark.suc', 'acronyms.txt', 'druggame.hum', 'deterior.hum', 'disaster.hum', 'art-fart.hum', 'dingding.hum', 'defectiv.hum', 'roadpizz.txt', 'murphy_l.txt', 'enquire.hum', 'engineer.hum', 'age.txt', 'bad-d', 'doggun.sto', 'beerdiag.txt', 'bad.jok', 'drinking.tro', 'firstaid.txt', 'record_.gap', 'finalexm.hum', 'footfun.hum', 'moose.txt', 'middle.age', 'flattax.hum', 'relative.ada', 'from.hum', 'freshman.hum', 'revolt.dj', 'resrch_p.hra', 'residncy.jok', 'forsooth.hum', 'men&wome.txt', 'mensroom.jok', 'fwksfun.hum', 'goforth.hum', 'female.jok', 'grommet.hum', 'bbq.txt', 'lposting.json', 'posting_doc.json']\n"
          ]
        }
      ],
      "source": [
        "#Loading the dataset\n",
        "file_list = os.listdir(r\"/content/drive/MyDrive/Datasets/Humor,Hist,Media,Food\")\n",
        "print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtMsdFfA7LFU",
        "outputId": "bdd5e02d-56df-4b48-f7b9-9c2205afba93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1135"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#Check the number of files present\n",
        "len(file_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-_c-rUgCreK"
      },
      "source": [
        "## **(a) Preprocessing Steps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZgbKMoMnomgX"
      },
      "outputs": [],
      "source": [
        "doc = {}\n",
        "lp = len(string.punctuation)\n",
        "#Cleaning the data\n",
        "def cleaning(flag, path, name):\n",
        "  #For pre-processing queries\n",
        "  if(flag==0):\n",
        "    #Expand contractions\n",
        "    #path = contractions.fix(path)\n",
        "    #Convert to lower case\n",
        "    path = path.lower()\n",
        "    #Remove punctuations and replace with space\n",
        "    path = path.translate(str.maketrans(string.punctuation, \" \"*lp, \"\"))\n",
        "    path = re.sub('[^A-Za-z\\s\\n ]+', ' ',path)\n",
        "    #Function for tokenization\n",
        "    path = word_tokenize(path)\n",
        "    #Function for stop words removal and lemmatization\n",
        "    path = [lema.lemmatize(s) for s in path if s not in stopwords.words('english') and s.isalpha and len(s)>1]\n",
        "    return path\n",
        "\n",
        "  else:\n",
        "    #For pre-processing of files\n",
        "    rf = open(path,'r',errors = 'ignore')\n",
        "    #Read the file\n",
        "    rf = rf.read()\n",
        "    #Convert to lower case\n",
        "    clean = rf.lower()\n",
        "    #Remove punctuations and replace with space\n",
        "    clean = clean.translate(str.maketrans(string.punctuation, \" \"*lp, \"\"))\n",
        "    clean = re.sub('[^A-Za-z\\s\\n ]+', ' ',clean)\n",
        "    #Function for tokenization\n",
        "    clean = word_tokenize(clean)\n",
        "    #Function for stop words removal and lemmatization\n",
        "    clean = [lema.lemmatize(s) for s in clean if s not in stopwords.words('english') and s.isalpha and len(s)>1]\n",
        "    doc[name] = clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MYf4T3hEYEMT"
      },
      "outputs": [],
      "source": [
        "for file in file_list:\n",
        "  cleaning(1,\"/content/drive/MyDrive/Datasets/Humor,Hist,Media,Food/\"+file,file)\n",
        "  #flag = 1 for pre-processing of files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0wn_7qoX7X10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14dd234-286a-4172-e2d5-e28a5f03ff96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1135"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY8dFWgFpy9l"
      },
      "source": [
        "## **(b) Unigram Inverted Index Data Structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Bho5D2oG7SzF"
      },
      "outputs": [],
      "source": [
        "invertedIndex = {}\n",
        "final = []\n",
        "#Final set of words\n",
        "file = open('vocabulary.txt','w')\n",
        "for doc_name in doc:\n",
        "    for word in doc[doc_name]:\n",
        "        final.append(word)\n",
        "final = set(final)\n",
        "for word in final:\n",
        "   file.write(word+\" , \")\n",
        "file.close\n",
        "#invertedIndex (dictinary) contains term and posting list\n",
        "#Build posting list containing all doc id's where term exist\n",
        "post = open('posting.txt','w') \n",
        "for term in final:\n",
        "    postingList = []\n",
        "    post.write(\"\\n\"+term + \" - \")\n",
        "    for id in doc:\n",
        "        if term in doc[id]:\n",
        "            postingList.append(id)\n",
        "            post.write(id+\" , \")\n",
        "    invertedIndex[term] = postingList "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "V47MgJ1xyjYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0b99c3-b93e-4ffd-c8d6-b108cd9b7428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/invertedIndex']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "#dump the invertedIndex using joblib\n",
        "joblib.dump(invertedIndex, '/content/drive/MyDrive/invertedIndex')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "44CD4r86ynpy"
      },
      "outputs": [],
      "source": [
        "#Load invertedIndex using joblib in postings\n",
        "invertedIndex = joblib.load('/content/drive/MyDrive/invertedIndex')\n",
        "postings = invertedIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g0MlYN0tWQs"
      },
      "source": [
        "## **(c) Query Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fSj1blL_7S3r"
      },
      "outputs": [],
      "source": [
        "#Pre-process queries and operators\n",
        "#Use flag = 0\n",
        "qp=[]\n",
        "def qprocess(operator, query):\n",
        "  #Pre-process the given operator \n",
        "  operator = re.sub(\"[^a-zA-Z\\s\\n\\,]\", \" \",operator)\n",
        "  op = operator.split(\",\")\n",
        "  op = [x.strip(\" \") for x in op]\n",
        "  op = [x.upper() for x in op]\n",
        "  #Pre-process the given query \n",
        "  qp = cleaning(0, query, \" \") \n",
        "  return op, qp "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CH4it0oR3Uh"
      },
      "source": [
        "## **(i)   x OR y**\n",
        "## **(ii)   x AND y** \n",
        "## **(iii)   x AND NOT y**\n",
        "## **(iv)   x OR NOT y** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UnkW0Yqw7S6k"
      },
      "outputs": [],
      "source": [
        "def merge_pl(op, qfinal):\n",
        "  c = 0\n",
        "  w1 = qfinal[0]\n",
        "  w2 = qfinal[1]\n",
        "  doc1=[]\n",
        "  doc2=[]\n",
        "  finalList=[]\n",
        "\n",
        "\n",
        "  for i in range(len(qfinal)-1):\n",
        "      #Take two words at a time, find posting list, add to the final doc and remove intersection\n",
        "      #Finding concatenation\n",
        "      if op[i] == 'OR':\n",
        "        if i == 0 :\n",
        "          if (w1 in postings.keys()):\n",
        "           doc1 = postings[w1].copy()\n",
        "          if (w2 in postings.keys()):\n",
        "           doc2 = postings[w2].copy()\n",
        "        else :\n",
        "          doc1=[]\n",
        "          doc1 = finalList\n",
        "          finalList = []\n",
        "          w2 = qfinal[i+1]\n",
        "          if (w2 in postings.keys()):\n",
        "              doc2=[]\n",
        "              doc2 = postings[w2].copy()\n",
        "        for item in doc1:\n",
        "            c += 1\n",
        "            finalList.append(item)\n",
        "            if item in doc2:\n",
        "              doc2.remove(item)\n",
        "        for item in doc2:\n",
        "          c += 1\n",
        "          finalList.append(item)\n",
        "\n",
        "      #Take two words at a time, find posting list, find intersection and add to the final doc \n",
        "      if op[i] == 'AND':\n",
        "        if i == 0 :\n",
        "          if (w1 in postings.keys()):\n",
        "           doc1 = postings[w1].copy()\n",
        "          if (w2 in postings.keys()):\n",
        "           doc2 = postings[w2].copy()\n",
        "        else :\n",
        "          doc1=[]\n",
        "          doc1 = finalList\n",
        "          finalList = []\n",
        "          w2 = qfinal[i+1]\n",
        "          if (w2 in postings.keys()):\n",
        "              doc2 = []\n",
        "              doc2 = postings[w2].copy()\n",
        "        for item in doc1:\n",
        "            if item in doc2:\n",
        "              c +=1\n",
        "              finalList.append(item)  \n",
        "\n",
        "      #Find posting list of two words, add posting list of w1 in final doc, remove intersection of w1 and w2 posting list          \n",
        "      if op[i] == 'AND NOT':\n",
        "        if i == 0 :\n",
        "          if (w1 in postings.keys()):\n",
        "           doc1 = postings[w1].copy()\n",
        "          if (w2 in postings.keys()):\n",
        "           doc2 = postings[w2].copy()\n",
        "          for item in doc1:\n",
        "            c += 1\n",
        "            finalList.append(item)\n",
        "            if item in doc2:\n",
        "              finalList.remove(item)           \n",
        "        else :\n",
        "          doc1=[]\n",
        "          doc1 = finalList\n",
        "          finalList = []\n",
        "          w2 = qfinal[i+1]\n",
        "          if (w2 in postings.keys()):\n",
        "              doc2=[]\n",
        "              doc2 = postings[w2].copy()\n",
        "          for item in doc1:\n",
        "            finalList.append(item)\n",
        "            if item in doc2:\n",
        "              finalList.remove(item)\n",
        "              c += 1\n",
        "\n",
        "      #Find posting list of w1 and find postings in which 2 is not present, then do OR operation\n",
        "      if op[i] == 'OR NOT':\n",
        "        if i == 0 :\n",
        "          if (w1 in postings.keys()):\n",
        "           doc1 = postings[w1].copy()\n",
        "          for item in postings.keys():\n",
        "             if item != w2:\n",
        "               for u in postings[item]:\n",
        "                  if u not in doc2:\n",
        "                    doc2.append(u)       \n",
        "        else :\n",
        "          doc1 = []\n",
        "          doc1 = finalList\n",
        "          finalList = []\n",
        "\n",
        "          w2 = qfinal[i+1]\n",
        "          doc2 = []\n",
        "          for item in postings.keys():\n",
        "             if item != w2:\n",
        "               for u in postings[item]:\n",
        "                    if u not in doc2:\n",
        "                       doc2.append(u)\n",
        "        for item in doc1:\n",
        "            c += 1\n",
        "            finalList.append(item)\n",
        "            if item in doc2:\n",
        "              doc2.remove(item)\n",
        "           \n",
        "        for item in doc2:\n",
        "          c += 1\n",
        "          finalList.append(item)\n",
        "\n",
        "\n",
        "  print(\"Number of documents matched: \", len(finalList))\n",
        "  print(\"No. of comparisons required: \", c)\n",
        "  print(finalList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICEUl4apRidN"
      },
      "source": [
        "## **(d) Evaluation of queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MtCuyIBiNnNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3f8d8b-1191-4d7b-bac8-395233349391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of queries: \n",
            "2\n",
            "Input query: \n",
            "lion stood thoughtfully for a moment\n",
            "Input operation sequence: \n",
            "OR, OR , OR\n",
            "Number of documents matched:  213\n",
            "No. of comparisons required:  414\n",
            "['murphys.txt', 'llong.hum', 'onetotwo.hum', 'deep.txt', 'drunk.txt', 'hecomes.jok', 'wagon.hum', 'lion.jok', 'puzzles.jok', 'lions.cat', 'tnd.1', 'boneles2.txt', 'dthought.txt', 'tpquotes.txt', 'japantv.txt', 'mindvox', 'lion.txt', 'booze1.fun', 'stuf10.txt', 'gd_gal.txt', 'christop.int', 'filmgoof.txt', 'dromes.txt', 'three.txt', 'bitnet.txt', 'collected_quotes.txt', 'computer.txt', 'murphy_l.txt', 'lposting.json', 'posting_doc.json', 'conan.txt', 'ivan.hum', 'lifeimag.hum', 'lozerzon.hum', 'luggage.hum', 'm0dzmen.hum', 'maecenas.hum', 'reasons.txt', 'montpyth.hum', 'news.hum', 'onetoone.hum', 'peatchp.hum', 'pizzawho.hum', 'phorse.hum', 'passage.hum', 'quest.hum', 'fuckyou2.txt', 'smurfkil.hum', 'soleleer.hum', 'stone.hum', 'lawyer.jok', 'marriage.hum', 'misc.1', 'xibovac.txt', 'childhoo.jok', 'nigel.5', 'pepsideg.txt', 'quux_p.oem', 'eskimo.nel', 'prac1.jok', 'prac3.jok', 'jc-elvis.inf', 'epitaph', 'quack26.txt', 'cogdis.txt', 'kanalx.txt', 'barney.txt', 'bmdn01.txt', 'flux_fix.txt', 'golnar.txt', 'iremember', 'nameisreo.txt', 'epi_.txt', 'epi_tton.txt', 'scratchy.txt', 'pracjoke.txt', 'cybrtrsh.txt', 'humor9.txt', 'cookie.1', 'minn.txt', 'practica.txt', 'homebrew.txt', 'insult.lst', 'insults1.txt', 'calculus.txt', 'indgrdn.txt', 'namaste.txt', 'lifeonledge.txt', 'mel.txt', 'grail.txt', 'mlverb.hum', 'ghostfun.hum', 'throwawa.hum', 'solders.hum', 'cabbage.txt', 'nihgel_8.9', 'cuchy.hum', 'engineer.hum', 'moose.txt', 'mash.hum', 'devils.jok', 'strine.txt', 'coyote.txt', 'incarhel.hum', 'cartoon_.txt', 'lbinter.hum', 'lif&love.hum', 'mailfrag.hum', 'meinkamp.hum', 'myheart.hum', 'cartoon.law', 'oldeng.hum', 'policpig.hum', 'popmusi.hum', 'pro-fact.hum', 'radiolaf.hum', 'shuttleb.hum', 'socecon.hum', 'top10.txt', \"terrmcd'.hum\", 'timetr.hum', 'tfepisod.hum', 'let.go', 'cartoon.laws', 'whoops.hum', 'wedding.hum', 'worldend.hum', 'wimptest.txt', 'commutin.jok', 'nigel.10', 'nigel.2', 'nigel.3', 'idr2.txt', 'exam.50', 'fascist.txt', 'passenge.sim', 'gas.txt', 'psych_pr.quo', 'pukeprom.jok', 'psycho.txt', 'letgosh.txt', 'luvstory.txt', 'prac4.jok', 'oxymoron.jok', 'progrs.gph', 'quotes.txt', 'bw-phwan.hat', 'ambrose.bie', 'anim_lif.txt', 'various.txt', 'vonthomp', 'gown.txt', 'snapple.rum', 'candy.txt', 'pepper.txt', 'drinks.gui', 'top10st2.txt', 'mundane.v2', 'sw_err.txt', 'econridl.fun', 'oliver.txt', 'oliver02.txt', 'classicm.hum', 'cmu.share', 'bbh_intv.txt', 'aeonint.txt', 'consp.txt', 'rns_ency.txt', 'stuf11.txt', 'sfmovie.txt', 'msorrow', 'episimp2.txt', 'gd_ql.txt', 'a_tv_t-p.com', 'allusion', 'anime.lif', 'wacky.ani', 'clancy.txt', 'facedeth.txt', 'nukewar.txt', 'petshop', 'prac2.jok', 'is_story.txt', 'english.txt', 'beauty.tm', 'initials.rid', 'annoy.fascist', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'hackmorality.txt', 'caesardr.sal', 'beesherb.txt', 'bw.txt', 'jayjay.txt', 'valujet.txt', 'mcd.txt', 'reeves.txt', 'moore.txt', 'chickenheadbbs.txt', 'ukunderg.txt', 'hackingcracking.txt', 'kaboom.hum', 'b-2.jok', 'suicide2.txt', 'nigel10.txt', 'badday.hum', 'butwrong.hum', 'manners.txt', 'art-fart.hum', 'dingding.hum', 'doggun.sto', 'female.jok']\n",
            "Input query: \n",
            "telephone,paved, roads\n",
            "Input operation sequence: \n",
            "OR NOT, AND NOT\n",
            "Number of documents matched:  998\n",
            "No. of comparisons required:  1272\n",
            "['cartoon_.txt', 'looser.hum', 'losers84.hum', 'cartoon.law', 'raven.hum', 'radiolaf.hum', 'drunk.txt', 'top10.txt', 'truthlsd.hum', 'cartoon.laws', 'wagon.hum', 'yjohncse.hum', 'jeffie.heh', 'final-ex.txt', 'smartass.txt', 'telecom.q', 'racist.net', 'ambrose.bie', 'bad', 'variety3.asc', 'wrdnws4.txt', 'wrdnws5.txt', 'kanalx.txt', 'transp.txt', 'bored.txt', 'sf-zine.pub', 'deathhem.txt', 'msorrow', 'nameisreo.txt', 'twilight.txt', 'christop.int', 'sorority.gir', 'pracjoke.txt', 'how2dotv.txt', 'hell.txt', 'c0dez.txt', 'hack7.txt', 'three.txt', 'acetab1.txt', 'proudlyserve.txt', 'jayjay.txt', 'beerwarn.txt', 'cartoon_laws.txt', 'jokes1.txt', 'fbipizza.txt', 'deadlysins.txt', 'planetzero.txt', 'crazy.txt', 'lifeonledge.txt', 'hacktest.txt', 'hackingcracking.txt', 'badday.hum', 'anthropo.stu', 'record_.gap', 'finalexm.hum', 'hop.faq', 't-10.hum', 'wisdom', 'buldrwho.txt', 'math.far', 'electric.txt', 'sysadmin.txt', 'thievco.txt', 'cuisine.txt', 'sawyer.txt', 'phxbbs-m.txt', 'appetiz.rcp', 'trekwes.hum', 'smiley.txt', 'top10st1.txt', 'top10st2.txt', 'nuke.hum', 'poll2res.hum', 'ratings.hum', 'vaguemag.90s', 'quick.jok', 'tpquote2.txt', 'acronym.txt', 'bw.txt', 'modest.hum', 'argotdic.txt', 'oculis.rcp', 'mindvox', 'pepper.txt', 'nysucks.hum', 'oasis', 'peatchp.hum', 'pukeprom.jok', 'luvstory.txt', 'rns_bwl.txt', 'epi_tton.txt', 'epi_rns.txt', 'dead4.txt', 'seeds42.txt', 'test.jok', 'ludeinfo.txt', 'ludeinfo.hum', 'kloo.txt', 'coffee.faq', 'twinpeak.txt', 'allusion', 'cast.lis', 'signatur.jok', 'dark.suc', 'catstory.txt', 'passage.hum', 'prawblim.hum', 'lines.jok', 'pickup.txt', 'penisprt.txt', 'rns_bcl.txt', 'gd_sgrnd.txt', 'skincat', 'murphy.txt', 'ripoffpc.hum', 'la_times.hun', 'fiber.txt', 'gd_hhead.txt', 'blake7.lis', 'booze.fun', 'get.drunk.cheap', 'pat.txt', 'gotukola.hrb', 'beesherb.txt', 'critic.txt', 'byfb.txt', 'cheapin.la', 'manspace.hum', 'policpig.hum', 'socecon.hum', 'talebeat.hum', 'fuck!.txt', 'startrek.txt', 'quotes.jok', 'shorties.jok', 'q.pun', 'ads.txt', 'oliver.txt', 'oliver02.txt', 'horflick.txt', 'cform2.txt', 'adrian_e.faq', 'wkrp.epi', 'arnold.txt', 'b-2.jok', 'cowexplo.hum', 'browneco.hum', 'coyote.txt', 'impurmat.hum', 'm0dzmen.hum', 'maecenas.hum', 'soleleer.hum', 'hecomes.jok', 'lion.jok', 'nigel.2', 'nigel.4', 'nigel.7', 'polly.txt', 'polly_.new', 'lions.cat', 'number.killer', 'wrdnws1.txt', 'epikarat.txt', 'lion.txt', 'lp-assoc.txt', 'macsfarm.old', 'number_k.ill', 'topten.hum', 'fireplacein.txt', 'brainect.hum', 'prover_w.iso', 'prover.wisom', 'howlong.hum', 'advrtize.txt', 'lost.txt', 'htswfren.txt', 'swearfrn.hum', 'arthriti.txt', 'antibiot.txt', 'ookpik.hum', 'woodbugs.txt', 'bagelope.txt', 'cooking.fun', 'classicm.hum', 'btscke01.des', 'cereal.txt', 'beave.hum', 'comrevi1.hum', 'pizzawho.hum', 'limerick.jok', 'horoscop.jok', 'luzerzo2.hum', 'tickmoon.hum', 'twinkie.txt', 'zodiac.hum', 'horoscop.txt', 'woodsmok.txt', 'variety2.asc', 'b12.txt', 'coke.txt', 'twinkies.jok', 'hangover.txt', 'sfmovie.txt', 'inquirer.txt', 'facedeth.txt', 'cokeform.txt', 'acne1.txt', 'horoscope.txt', 'cancer.rat', 'x-drinks.txt', 'shooters.txt', 'drinks.txt', 'booze1.fun', 'booze2.fun', 'nzdrinks.txt', 'herb!.hum', 'murphys.txt', 'legal.hum', 'llong.hum', 'mtm.hum', 'devils.jok', 'climbing.let', 'netnews.10', 'eandb.drx', 'elephant.fun', 'lawskool.txt', 'psych_pr.quo', 'insuranc.sty', 'law.sch', 'inlaws1.txt', 'laws.txt', 'one.par', 'primes.jok', 'princess.brd', 'tpquotes.txt', 'wrdnws9.txt', 'old.txt', 'necropls.txt', 'lawyers.txt', 'murphy_l.txt', 'engineer.hum', 'hell.jok', 'lll.hum', 'lifeinfo.hum', 'lozeuser.hum', 'mash.hum', 'novel.hum', 'quest.hum', 'research.hum', 'rocking.hum', 'insult', 'terbear.txt', 'whoon1st.hum', 'why-me.hum', 'st_silic.txt', 'wimptest.txt', 'childhoo.jok', 'gd_guide.txt', 'pournell.spo', 'idaho.txt', 'psycho.txt', 'jargon.phd', 'lampoon.jok', 'humpty.dumpty', 'resrch_phrase', 'tnd.1', 'epitaph', 'various.txt', 'mrsfield', 'choco-ch.ips', 'barney.txt', 'bmdn01.txt', 'msfields.txt', 'food', 'red-neck.jks', 'fusion.gal', 'outlimit.txt', 'epiquest.txt', 'gd_ol.txt', 'hum2', 'crzycred.lst', 'butcher.txt', 'aniherb.txt', 'trekfume.txt', 'bhb.ill', 'indgrdn.txt', 'abbott.txt', 'd-ned.hum', 'resrch_p.hra', 'forsooth.hum', 'fwksfun.hum', 'turkey.fun', 'rns_ency.txt', 'mead.rcp', 'aeonint.txt', 'damiana.hrb', 'a_tv_t-p.com', 'ohandre.hum', 'turing.shr', 'realest.txt', 'jac&tuu.hum', 'lifeimag.hum', 'lif&love.hum', 'manilla.hum', 'planeget.hum', 'popconc.hum', 'prayer.hum', 'fuckyou2.txt', 'spydust.hum', 'koans.txt', 'tshirts.jok', 'zgtoilet.txt', 'nigel.10', 'proof.met', 'psalm23.txt', 'psalm_re.aga', 'leech.txt', 'jokeju07.txt', 'oxymoron.jok', 'psalm.reagan', 'psalm_nixon', 'bw-phwan.hat', 'supermar.rul', 'oldtime.sng', 'talkbizr.txt', 'vegkill.txt', 'vegan.rcp', 'smurf-03.txt', 'diet.txt', 'how2bgod.txt', 'bread.rec', 'docdict.txt', 'golnar.txt', 'dalive', 'epi_merm.txt', 'wacky.ani', 'nukewar.txt', 'resolutn.txt', 'all_grai', 'brush1.txt', '1st_aid.txt', 'back1.txt', 'basehead.txt', 'draxamus.txt', 'grail.txt', 'bingbong.hum', 'parades.hum', 'zen.txt', 'atombomb.hum', 'nigel10.txt', 'bank.rob', 'beapimp.hum', 'making_y.wel', 'college.sla', 'defectiv.hum', 'bad.jok', 'normquot.txt', 'computer.txt', 'insanity.hum', 'lipkovits.txt', 'proposal.jok', 'wetdream.hum', 'eskimo.nel', 'oxymoron.txt', 'chili.txt', 'kashrut.txt', 'fajitas.rcp', 'homermmm.txt', 'eatme.txt', 'wonton.txt', 'beershrm.fis', 'beershrp.fis', 'shuimai.txt', 'seafood.txt', 'blood.txt', 'gumbo.txt', 'butwrong.hum', 'chunnel.txt', 'nintendo.jok', 'outawork.erl', 'the_ant.txt', 'elevator.fun', 'teevee.hum', 'cheapfar.hum', 'spacever.hum', 'univ.odd', 'coke.fun', 'beginn.ers', 'how.bugs.breakd', 'bugs.txt', 'gd_flybd.txt', 'renorthr.txt', 'terms.hum', 'missheav.hum', 'terrnieg.hum', 'math.2', 'jokes.txt', 'grammar.jok', 'coladrik.fun', 'coladrik.txt', 'mog-history', 'lozers', 'investi.hum', 'cbmatic.hum', 'boe.hum', 'mailfrag.hum', 'jrrt.riddle', 'lazarus.txt', 'cybrtrsh.txt', 'blackhol.hum', 'namm', 'earp', 'hi.tec', 'fartinfo.txt', 'curse.txt', 'suicide2.txt', 'spelin_r.ifo', 'marriage.hum', 'english', 'dromes.txt', 'drugshum.hum', 'reconcil.hum', 'gd_gal.txt', 'religion.txt', 'fusion.sup', 'blooprs1.asc', 'thermite.ana', 'coollngo2.txt', 'thesis.beh', 'wagit.txt', 'letterbx.txt', 'sw_err.txt', 'hbo_spec.rev', 'goforth.hum', 'miami.hum', 'gas.txt', 'chickenheadbbs.txt', 'hotnnot.hum', 'news.hum', 'reagan.hum', 'quotes.bug', 'gd_liqtv.txt', 'humatran.jok', 'humatra.txt', 'jerky.rcp', 'dieter.txt', 'psilaine.hum', 'donut.txt', 'smokers.txt', 'gameshow.txt', 'wrdnws8.txt', 'kilroy', 'gd_drwho.txt', 'amchap2.txt', 'merry.txt', 'rabbit.txt', 'failure.txt', 'disaster.hum', 'paddingurpapers.txt', 'conan.txt', 'woolly_m.amm', 'men&wome.txt', 'addrmeri.txt', 'charity.hum', 'bitchcar.hum', 'blackapp.hum', 'misery.hum', 'growth.txt', 'episimp2.txt', 'headlnrs', 'anorexia.txt', 'llamas.txt', 'packard.txt', 'snipe.txt', 'simp.txt', 'truths.hum', 'mutate.hum', 'english.txt', 'murph.jok', 'squids.gph', 'cucumber.txt', \"terrmcd'.hum\", 'cucumber.jok', 'kilsmur.hum', 'smurfs.cc', 'hate.hum', 'phunatdi.ana', 'oldeng.hum', 'phony.hum', 'shrink.news', 'cogdis.txt', 'mtv.asc', 'gd_maxhd.txt', 'italoink.txt', 'gown.txt', 'losers86.hum', 'beer.gam', 'booknuti.txt', 'cabbage.txt', 'adameve.hum', 'pipespec.txt', 'hitlerap.txt', 'odd_to.obs', 'mydaywss.hum', 'cartwb.son', 'drinkrul.jok', 'co-car.jok', 'enlightenment.txt', 'popmach', 'atherosc.txt', 'taping.hum', 'stressman.txt', 'imprrisk.hum', 'voltron.hum', 'lawhunt.txt', 'livnware.hum', 'myheart.hum', 't_zone.jok', 'skippy.hum', 'wedding.hum', 'hotel.txt', 'skippy.txt', 'econridl.fun', 'gd_frasr.txt', 'pot.txt', 'bbc_vide.cat', 'awespinh.sal', 'jalapast.dip', 'adcopy.hum', 'boatmemo.jok', 'bbq.txt', 'cmu.share', 'church.sto', 'thecube.hum', 'bnb_quot.txt', 'test.hum', 'bread.txt', 'dym', 'texican.dic', 'texican.lex', 'ivan.hum', 'office.txt', 'madscrib.hum', 'meinkamp.hum', 'mrscienc.hum', 'repair.hum', 'televisi.hum', 'math.1', 'passenge.sim', 'moonshin', 'hitler.59', 'televisi.txt', 'speling.msk', 'anim_lif.txt', 'widows', 'coffee.txt', 'beer-g', 'beergame.hum', 'anime.lif', 'chung.iv', 'fed.txt', 'poopie.txt', 'minn.txt', 'pasta001.sal', 'aphrodis.txt', 'att.txt', 'symbol.hum', 'art-fart.hum', 'dingding.hum', 'relative.ada', 'freshman.hum', 'residncy.jok', 'excuse30.txt', 'valujet.txt', 'mel.txt', 'interv.hum', 'miranda.hum', 'sungenu.hum', 'word.hum', 'fascist.txt', 'mr.rogers', 'tuna.lab', 'hitchcoc.app', 'annoy.fascist', 'airlines', 'alflog.txt', 'female.jok', 'alcohol.hum', 'modstup', 'let.go', 'billcat.hum', 'letgosh.txt', 'letter_f.sch', 'hitchcok.txt', 'hermsys.txt', 'saveface.hum', 'yogisays.txt', 'girlspeak.txt', 'timetr.hum', 'yuban.txt', 'radexposed.txt', 'wood', 'japrap.hum', 'luggage.hum', 'docspeak.txt', 'college.hum', 'doc-says.txt', 'happyhack.txt', 'maxheadr', 'clancy.txt', 'sinksub.txt', 'gingbeer.txt', 'cockney.alp', 'rentals.hum', 'farsi.phrase', 'farsi.txt', 'arab.dic', 'mov_rail.txt', 'ghostsch.hum', 'netmask.txt', 'robot.tes', 'makebeer.hum', 'curry.hrb', 'fish.rec', 'penndtch', 'curry.txt', 'tuflife.txt', 'testchri.txt', 'test2.jok', 'standard.hum', 'nukwaste', 'madhattr.jok', 'flattax.hum', 'yuppies.hum', 'exam.50', 'calculus.txt', 'pun.txt', 'fartting.txt', 'rent-a_cat', 'spoonlis.txt', 'lansing.txt', 'studentb.txt', 'stereo.txt', 'engrhyme.txt', 'confucius_say.txt', 'moore.txt', 'newcoke.txt', 'fearcola.hum', 'sysman.txt', 'parsnip.txt', 'contract.moo', 'college.txt', 'history2.oop', 'ayurved.txt', 't-shirt.hum', 'caesardr.sal', 'libraway.txt', 'post.nuc', 'appbred.brd', 'banana01.brd', 'banana03.brd', 'breadpud.des', 'bredcake.des', 'nukewar.jok', 'stone.hum', 'court.quips', 'bozo_tv.leg', 'popmusi.hum', 'snapple.rum', 'motrbike.jok', 'calvin.txt', 'un.happy', 'foodtips', 'curiousgeorgie.txt', 'coffeebeerwomen.txt', 'bad-d', 'rinaldos.txt', 'mothers.txt', 'lawsuniv.hum', 'kid_diet.txt', 'phorse.hum', 'social.hum', 'teens.txt', 'toxcwast.hum', 'worldend.hum', 'xtermin8.hum', 'who.txt', 'hack', 'drinker.txt', 'grospoem.txt', 'rec.por', 'polemom.txt', 'rinaldos.law', 'firstaid.inf', 'cooking.jok', 'rinaldo.jok', 'bimg.prn', 'sanshop.txt', 'smurf_co.txt', 'cake.rec', 'petshop', 'feggmagi.txt', 'ppbeer.txt', 'figure_1.txt', 'beer.txt', 'freudonseuss.txt', 'eggroll1.mea', 'egglentl.vgn', 'applepie.des', 'banana05.brd', 'blkbnsrc.vgn', 'btscke04.des', 'bunacald.fis', 'oldtime.txt', 'recipe.007', 'beergame.txt', 'hitler.txt', 'btaco.txt', 'qttofu.vgn', 'aggie.txt', 'bond-2.txt', 'quantity.001', 'namaste.txt', 'exylic.txt', 'onan.txt', 'analogy.hum', 'ghostfun.hum', 'kaboom.hum', 'alabama.txt', 'catin.hat', 'beer.hum', 'bigpic1.hum', 'cold.fus', 'bugbreak.hum', 'change.hum', 'coldfake.hum', 'deterior.hum', 'beerdiag.txt', 'drinking.tro', 'firstaid.txt', 'insure.hum', 'killer.hum', 'report.hum', 'dead-r', 'druggame.hum', 'watchlip.hum', 'revolt.dj', 'socks.drx', 'fegg!int.txt', 'free-cof.fee', 'heroic.txt', 'memory.hum', 'shuttleb.hum', 'chickens.txt', 'reasons.txt', 'o-ttalk.hum', 'solviets.hum', 'welfare.txt', 'weight.txt', 'welfare', 'initials.rid', 'jawsalad.fis', 'jawgumbo.fis', 'mitch.txt', 'strattma.txt', 'margos.txt', 'zucantom.sal', 'german.aut', 'aussie.lng', 'roach.asc', 'p-law.hum', 'ratspit.hum', 'reddye.hum', 'wrdnws2.txt', 'cgs_lst.txt', 'cooplaws', 'ganamembers.txt', 'apsnet.txt', 'weights.hum', 'textgrap.hum', 'miamimth.txt', 'newconst.hum', 'bless.bc', 'opinion.hum', 'rapmastr.hum', 'whoops.hum', 'quantum.jok', 'moslem.txt', 'quantum.phy', 'mcd.txt', 'shameonu.hum', 'chickens.jok', 'gohome.hum', 'harmful.hum', 'incarhel.hum', 'naivewiz.hum', 'nurds.hum', 'deep.txt', 'parabl.hum', 'spider.hum', 'termpoem.txt', 'trukdeth.txt', 'fudge.txt', 'dthought.txt', 'slogans.txt', 'staff.txt', 'number', 'diesmurf.txt', 'jon.txt', 'whatbbs', 'brdpudd.des', 'jimhood.txt', 'exidy.txt', 'poets.hum', 'answers', 'btcisfre.hum', 'age.txt', 'doggun.sto', 'middle.age', 'grommet.hum', 'imbecile.txt', 'johann', 'marines.hum', 'lucky.cha', 'element.jok', 'manager.txt', 'newmex.hum', 'women.jok', 'soccer.txt', 'bhang.fun', 'oam-001.txt', 'beer-gui', 'chinese.txt', 'woodbine.txt', 'puzzle.spo', 'coke_fan.naz', 'bb', 'beerjesus.hum', 'silverclaws.txt', 'normalboy.txt', 'normal.boy', 'pepsideg.txt', 'banana02.brd', 'insect1.txt', 'yogurt.asc', 'temphell.jok', 'good.txt', 'bakebred.txt', 'egg-bred.txt', 'engmuffn.txt', 'batrbred.txt', 'richbred.txt', 'waitress.txt', 'chainltr.txt', 'adt_miam.txt', 'pickup.lin', 'height.txt', 'candybar.fun', 'godmonth.txt', 'dover.poem', 'mowers.txt', 'peanuts.txt', 'barney.cn1', 'how_to_i.pro', 'liceprof.sty', 'potty.txt', 'empeval.txt', 'mensroom.jok', 'miamadvi.hum', 'just2', 'catranch.hum', 'coke1', 'hamburge.nam', 'iced.tea', 'nasaglenn.txt', 'brownie.rec', 'feista01.dip', 'garlpast.vgn', 'apsaucke.des', 'banana04.brd', 'berryeto.bev', 'blkbean.txt', 'btscke02.des', 'burrito.mea', 'btscke05.des', 'recipe.011', 'recipe.005', 'recipe.012', 'odearakk.hum', 'tribble.hum', 'prooftec.txt', 'justify', 'reddwarf.sng', 'pbcookie.des', 'malechem.txt', 'iremember', 'avengers.lis', 'whatthe.hum', 'a_fish_c.apo', 'units.mea', 'pol-corr.txt', 'chinesec.hum', 'hedgehog.txt', 'memo.hum', 'corporat.txt', 'soporifi.abs', 'y.txt', 'buzzword.hum', 'caramels.des', 'solders.hum', 'washroom.txt', 'gaiahuma', 'cookbkly.how', 'ins1', 'cookberk', 'takenote.jok', 'idr2.txt', 'pecker.txt', 'turbo.hum', 'pure.mat', 'vonthomp', 'kid2', 'alcatax.txt', 'childrenbooks.txt', 'lobquad.hum', 'jungjuic.bev', 'blaster.hum', 'catballs.hum', 'calif.hum', 'footfun.hum', 'labels.txt', 'japice.bev', 'f_tang.txt', 'iqtest', 'recipe.002', 'venison.txt', 'hotpeper.txt', 'unochili.txt', 'arcadian.txt', 'jambalay.pol', 'puzzles.jok', 'steroid.txt', 'top10.elf', 'oatbran.rec', 'greenchi.txt', 'zuccmush.sal', 'boarchil.txt', 'hilbilly.wri', 'gack!.txt', 'problem.txt', 'recip1.txt', 'recipe.008', 'admin.txt', 'python_s.ong', 'jc-elvis.inf', 'venganza.txt', 'calamus.hrb', 'capital.txt', 'focaccia.brd', 'recipe.006', 'renored.txt', 'cops.txt', 'oranchic.pol', 'oakwood.txt', 'stagline.txt', 'icm.hum', 'recipe.004', 'ourfathr.txt', 'poli_t.ics', 'poli.tics', 'recipe.001', 'hierarch.txt', 'butstcod.fis', 'whitbred.txt', 'baklava.des', 'recipe.010', 'dandwine.bev', 'antimead.bev', 'btscke03.des', 'oam.nfo', 'orgfrost.bev', 'woods.txt', 'recipe.009', 'languag.jok', 'recepies.fun', 'buffwing.pol', 'texbeef.txt', 'desk.txt', 'firecamp.txt', 'bw-summe.hat', 'montoys.txt', 'recipe.003', 'frogeye1.sal', 'goldwatr.txt', 'flowchrt.txt', 'flowchrt', 'bible.txt', 'boston.geog']\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of queries: \")\n",
        "N = int(input())\n",
        "for j in range(N):\n",
        "  print(\"Input query: \")\n",
        "  query = input()\n",
        "  print(\"Input operation sequence: \")\n",
        "  operation = input()\n",
        "  op, qfinal = qprocess(operation, query)\n",
        "  merge_pl(op, qfinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPA3sVfG9UY1"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Z9qSJk2z91wH"
      },
      "outputs": [],
      "source": [
        "#Load the dataset\n",
        "path = \"/content/drive/MyDrive/Datasets/Humor,Hist,Media,Food\"\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtLdYrDZIiJ8"
      },
      "source": [
        "## **(a) Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gbZhHsyp9gDQ"
      },
      "outputs": [],
      "source": [
        "ln = len(string.punctuation)\n",
        "#Cleaning the data\n",
        "def process(content):\n",
        "  #Convert the text to lower case\n",
        "  content = content.lower()\n",
        "  #Remove punctuation marks from tokens\n",
        "  content = content.translate(str.maketrans(string.punctuation, \" \"*ln,''))\n",
        "  #Perform word tokenization\n",
        "  ctokens = word_tokenize(content)\n",
        "  #Remove stopwords from tokens and do lemmatization\n",
        "  #Checking length, if length = 1\n",
        "  ctokens = [lema.lemmatize(s) for s in ctokens if s not in stopwords.words('english') and s.isalpha and len(s)>1]\n",
        "  return ctokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-D82BQ86LuM"
      },
      "source": [
        "## **(b) Positional index data structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "zneYp3cEZxMq"
      },
      "outputs": [],
      "source": [
        "#Function to create posting list\n",
        "#Positional index\n",
        "pol={}\n",
        "def lposting(c, tl):\n",
        "  i = 0\n",
        "  for t in tl:\n",
        "    i = i+1\n",
        "    if t in pol:\n",
        "      d = pol[t][1]\n",
        "      if c in d:\n",
        "        d[c].append(i)\n",
        "      else:\n",
        "        pol[t][0] = (pol[t][0]+1)\n",
        "        d[c] = [i]\n",
        "      pol[t][1] = d\n",
        "    else:\n",
        "      pol[t] = [] \n",
        "      pol[t].append(1)\n",
        "      pol[t].append({})\n",
        "      pol[t][1][c] = [i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Rql8JtkwZxRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8b04e8-dbb7-48a6-961e-7513e77572c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1137\n"
          ]
        }
      ],
      "source": [
        "#Iterate over the dataset for preprocessing and indexing\n",
        "c = 0\n",
        "for file in glob.glob(\"*\"):\n",
        "  c = c+1\n",
        "  #Reading the file\n",
        "  with open(file,'r',encoding = 'utf8', errors = 'ignore') as f:\n",
        "    rfile = f.read()\n",
        "    doc[c] = file\n",
        "    tl = process(rfile)\n",
        "    lposting(c,tl)\n",
        "\n",
        "print(c)    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print terms in posting list\n",
        "for t in pol:\n",
        "  print(t,pol[t][0],len(pol[t][1]))"
      ],
      "metadata": {
        "id": "yKx_NPbJ7AfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MsxT7DOqo7lH"
      },
      "outputs": [],
      "source": [
        "#Dumping the posting index to avoid making posting index from scratch every time\n",
        "save = open('lposting.json','w')\n",
        "save = json.dump(pol,save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "17K-rag0o7ph"
      },
      "outputs": [],
      "source": [
        "#Loading the posting index\n",
        "load_file = open('posting_doc.json','w')\n",
        "load_file = json.dump(pol,load_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VaVQbnTC0dr"
      },
      "source": [
        "## **(c) Support for the searching of phrase queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "fW4OD0g3wj_K"
      },
      "outputs": [],
      "source": [
        "#Checking common documents for the query in the list\n",
        "def dcommon(qf):\n",
        "  #Store common documents\n",
        "  common = []\n",
        "  n = len(qf)\n",
        "  l = pol[qf[0]][1]\n",
        "  for d in l:\n",
        "    c=1\n",
        "    for i in range(1, n):\n",
        "      if d in pol[qf[i]][1]:\n",
        "        c = c+1\n",
        "    if c == n:\n",
        "      common.append(d)\n",
        "  return common  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SqDYxT9RwkBQ"
      },
      "outputs": [],
      "source": [
        "#Retrieving the list of positions for words from the common documents\n",
        "def word_pos(qf,cd):\n",
        "  pol1 = {}\n",
        "  for d in cd:\n",
        "    for t in qf:\n",
        "      if d in pol1:\n",
        "        pol1[d].append(pol[t][1][d])\n",
        "      else:\n",
        "        pol1[d] = [pol[t][1][d]]\n",
        "\n",
        "  return pol1    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmD3BMHYFvQs"
      },
      "source": [
        "## **(d) Query processing and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "hXXguD5vwkDP"
      },
      "outputs": [],
      "source": [
        "#Function for query processing\n",
        "def qprocess(qf):\n",
        "  f_doc = []\n",
        "  print(qf) \n",
        "  common = dcommon(qf)\n",
        "  plist1 = word_pos(qf,common)\n",
        "  for d in common:\n",
        "    f = False\n",
        "    tp = plist1[d]\n",
        "    tp1 = tp[0]\n",
        "    for i in range(len(tp1)):\n",
        "      f1 = True\n",
        "      c = tp1[i]\n",
        "      for j in range(1,len(tp)):\n",
        "        c = c+1\n",
        "        if c not in tp[j]:\n",
        "          f1 = False\n",
        "          break\n",
        "      if(f1):\n",
        "        f = True\n",
        "    if(f):\n",
        "      f_doc.append(d)\n",
        "    f_doc = list(set(f_doc))  \n",
        "  return f_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xjwIWxo7Dbkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0345ff1f-7193-4b02-8792-f29a2a76ca68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Query\n",
            "Good Day\n",
            "['good', 'day']\n",
            "\n",
            "The number of documents retrieved : 9\n",
            "The list of document names retrieved : \n",
            "['xibovac.txt', 'tnd.1', 'onetoone.hum', 'horoscop.jok', 'horoscop.txt', 'lozeuser.hum', 'humor9.txt', \"terrmcd'.hum\", 'valujet.txt']\n"
          ]
        }
      ],
      "source": [
        "#Evaluate query\n",
        "print(\"Enter Query\")\n",
        "#Input the query\n",
        "query = input()\n",
        "#preprocessing input query\n",
        "qf = process(query)\n",
        "#Processing input query\n",
        "d = qprocess(qf)\n",
        "#Listing documents\n",
        "final_doc = []\n",
        "for i in d:\n",
        "  final_doc.append(doc[i])\n",
        "\n",
        "print(f\"\\nThe number of documents retrieved : {len(d)}\")\n",
        "print(\"The list of document names retrieved : \")\n",
        "print(final_doc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR2022_A1_78.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}